---
title: "Practical Machine Learning"
author: "Romain Jacotin"
date: "26 avril 2015"
output: html_document
---

# Course project

This report is a R markdown file describing my analysis.

### Table of contents

* [Tidy data](#tidy)
* [Cross validation](#cross)
* [Random Forest model](#model)
* [Model evaluation](#eval)
* [Make the prediction submission](#submit)

## <A name="tidy"></A> Tidy data

We read the original training data and testing data and keep only the Tidy data by:

* Removing from the data set all the columns that contain 'NA' values
* Removing from the training set all the rows that contain 'NA' values
* Removing not useful features for predictions like "user_name", etc...

```{r}
# Reading data
training <- read.csv("pml-training.csv")
testing  <- read.csv("pml-testing.csv")

# Cleaning data

# Removing all features containing 'NA' values int training and/or testing data set
n            <- dim(training)[2]
featureIndex <- rep(TRUE,n)
for (i in 1:n ) {
    featureIndex[i] <- (sum(is.na( testing[,i] ))>0) || (sum(is.na( training[,i] ))>0)
}

# Removing not useful features:
# "X"
featureIndex[1] <- TRUE
# "user_name"
featureIndex[2] <- TRUE
# "raw_timestamp_part_1"
featureIndex[3] <- TRUE
# "raw_timestamp_part_2"
featureIndex[4] <- TRUE
# "cvtd_timestamp"
featureIndex[5] <- TRUE
# "new_window"
featureIndex[6] <- TRUE
# "num_window"
featureIndex[7] <- TRUE

# Cut each training and testing data sets in two : the dirty features part and the tidy features part
testingNA  <- testing[ , featureIndex ]
trainingNA <- training[ , featureIndex ]

testingTidy  <- testing[ , !featureIndex ]
trainingTidy <- training[ , !featureIndex ]
```

## <A name="cross"></A> Cross validation

For cross validation, let's partition the original data into 70% for the Training set and 30% for the Testing set (others cross validation algorithm like k-fold are more useful on smaller data set than what we have here, with so much observations we have less risk of overfitting in our case).

```{r}
library(caret)
set.seed(42)
inTrain <- createDataPartition( trainingTidy$classe, p=0.7, list=FALSE )
train   <- trainingTidy[ inTrain, ]
test    <- trainingTidy[ -inTrain, ]
```

Let's show the 5 classes repartition (A,B,C,D and E) in the newly created training (70%) and testing set (30%):

```{r}
table(train$class)
table(test$class)
```

## <A name="model"></A> Random Forest model

We can now train a Random Forest model on the training set. (Random Forest is actually the more use of all classification models, and win a lot of Kaggle competitions).

```{r}
modFit  <- train( classe~., data=train, method="rf" )
```

## <A name="eval"></A> Model evaluation

To evaluate the Random Forests model we need to use it for making predictions on the training set and the testing set.

```{r}
predTrain <- predict( modFit, train )
predTest  <- predict( modFit, test )
```

Then we can evaluate the Random Forests model by calculating the accuracy of this model on the training set and on the testing set:

```{r}
confusionMatrix( predTrain, train$classe )
confusionMatrix( predTest, test$classe )
```

| Data set | Training set ( 70% ) | Testing set ( 30% ) |
|---|---|---|
| Accuracy | xx.xxx %  |  xx.xxx% |

The Random Forest model has a very good accuracy of xx.xxx % on the testing set !

## <A name="submit"></A> Make the prediction submission

Now we use this Random Forests model to predict the dedicated testing set (only 20 evaluations) for the submission part of the course project:

```{r}
predSubmit <- predict( modFit, testingTidy )
```

And we create the 20 independent files needed for the manual submission on Coursera:

```{r}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

submitData <- as.character(predSubmit)
pml_write_files(submitData)
```

